#!/usr/bin/env python
"""
Commmand to launch baobab ntuple production
"""
import argparse
import sys
import re
import glob
import os
import os.path
import pwd
import socket

try:
    from CRABAPI.RawCommand import crabCommand
except ImportError:
    sys.stderr.write("\nCRAB 3 environment needs to be set before running this script and before setting the CMSSW environment (cmsenv or scram runtime).\n\n")
    sys.exit(1)

from httplib import HTTPException
from WMCore.Configuration import Configuration

##############################################

hostname = socket.gethostname()
print "\nhostname = %s" % hostname
eos_mount_point="/eos/cms/"
print "eos_mount_point = %s" % eos_mount_point


def getLumiMaskJsonFile(year):
    """Gets the input json file indicating the certified luminosity section which can be processed."""

    CMSSW = os.environ["CMSSW_BASE"]
    input_json_file = ""
    if (year == 2016):
        # 2016 Golden JSON
        input_json_file += CMSSW + "/src/WJetsTreeProducer/ntuple_production/datacards/Cert_271036-284044_13TeV_ReReco_07Aug2017_Collisions16_JSON.txt"
    elif (year == 2017):
        # 2017 Golden JSON
        input_json_file += CMSSW + "/src/WJetsTreeProducer/ntuple_production/datacards/Cert_294927-306462_13TeV_EOY2017ReReco_Collisions17_JSON.txt"
    else:
        # No Golden JSON for 2018 yet!!!
        input_json_file += CMSSW + ""
    print "input_json_file = %s" % input_json_file

    candidates = glob.glob(input_json_file)
    latest_ts = 0
    latest_file = None
    for f in candidates:
        ts = os.path.getmtime(f)
        if ts > latest_ts:
            latest_file = f
            latest_ts = ts
    if not latest_file:
        raise RuntimeError("JSON file required to process real data was not found. We have looked for files named %s" % input_json_file)
    return latest_file

def checkDataSetName(dataset):
    a = dataset.split("/")
    if len(a) != 4:
        sys.stderr.write("Dataset name must contains three part /primary_dataset/conditions/tier")
        return False
    p = re.compile("MINIAOD(|SIM)")
    if not p.match(a[3]):
        sys.stderr.write("Only MINIAOD and MINIOADSIM data tiers are supported.")
        return False
    return True

def getCrabConfig(dataset, iDataset, unitsPerJob, year, outDir, maxEvents):
    """Build the crab configuration file."""
    
    dataset_tier = dataset.split("/")[3]

    if dataset_tier.find("SIM") > 0:
        isMC = True
        print "Input dataset is recognized as MC"
    else:
        isMC = False
        print "Input dataset is NOT recognized as MC"
    #endif
    
    config = Configuration()

    config.section_('General')
    #----------------------------------------------------------------------

    #Request name. Used to identify job batch and to name the result directory    
    if isMC:
        config.General.requestName = str(year) + "_" + dataset.split('/')[1]
    else:
        config.General.requestName = str(year) + "_" + dataset.split('/')[1]+"_"+dataset.split("/")[2]

    #Switch for the copy of the results to the storage site.
    config.General.transferOutputs = True

    #Whether or not to copy the job log files to the storage site.
    # config.General.transferLogs = True
    config.General.transferLogs = False

    config.section_('JobType')
    #----------------------------------------------------------------------

    # Running an analysis over a data set
    config.JobType.pluginName = 'Analysis'

    # CMSS Configuration file
    config.JobType.psetName = 'grow_baobabs_cfg.py'

    # Specify options for cfg file (as one would on command line)
    # Change the year according to what dataset you are processing!
    if isMC:
        config.JobType.pyCfgParams = ["maxEvents=-1", "year="+str(year), "isData=0", "doGenInfo=0"]
    else:
        config.JobType.pyCfgParams = ["maxEvents=-1", "year="+str(year), "isData=1", "doGenInfo=0"]

    # Increase required time to run jobs (defaults to 1315)
    config.JobType.maxJobRuntimeMin = 2400
    #config.JobType.maxJobRuntimeMin = 1315 #based on crab logs for current event submission numbers

    # Increase maximum memory (in MB) a job is allowed to use
    #config.JobType.maxMemoryMB = 2500
    config.JobType.maxMemoryMB = 2100 #based on crab logs for current event submission numbers

    config.section_('Data')
    #----------------------------------------------------------------------
    # Name of the dataset to analyze
    config.Data.inputDataset = dataset

    # List of certified luminosity section, which can be processed (applies to real data only)
    if not isMC:
        config.Data.lumiMask = getLumiMaskJsonFile(year)
        # print ("Using this JSON file as lumi mask: "+str(input_json_file))

    # Output directory on the storage site (storage site defined in the Site section)
    print "Output Dir: %s\n" % outDir.split(":")[1]
    config.Data.outLFNDirBase = outDir.split(":")[1]

    #Mode to use to split the task in jobs. 
    # for 'Analysis' job type: 'FileBased', 'LumiBased' (recommended for real data), or 'EventAwareLumiBased'
    # for 'PrivateMC' job type: 'EventBased'
    if isMC:
        config.Data.splitting = 'FileBased'
    else:
        config.Data.splitting = 'EventAwareLumiBased'

    # Number of units to produce or process
    if maxEvents and maxEvents > 0:
        config.Data.totalUnits = maxEvents
        if maxEvents < unitsPerJob:
            unitsPerJob = maxEvents

    # Number of units (events in case of PrivateMC)  per job
    config.Data.unitsPerJob = unitsPerJob

    # Switch for publication of output data to DBS
    config.Data.publication = False

    config.section_('User')
    #----------------------------------------------------------------------
    
    config.section_('Site')
    #----------------------------------------------------------------------

    #A list of sites where the jobs should not run.
    config.Site.blacklist = ['T3_TW_NTU_HEP', 'T3_GR_IASA', 'T2_GR_Ioannina', 'T3_MX_Cinvestav', 'T2_DE_RWTH', 'T2_UK_SGrid_RALPP', 'T3_RU_FIAN', 'T2_FI_HIP', 'T2_BR_SPRACE', 'T2_ES_CIEMAT', 'T2_EE_Estonia', 'T3_UK_London_RHUL', 'T2_US_UCSD', 'T2_US_Caltech', 'T2_UK_London_IC', 'T3_UK_London_QMUL']
    # last two in this list because they can't handle CMSSW_9_4_15 well I think
    
    #This does not seem to be a good idea since not all the files are in the US sites.
    #config.Site.whitelist = ["T2_US*","T3_US*"]

    #Site where the output should be copied. 
    # See https://cmsweb.cern.ch/sitedb/prod/sites/ for the list of site names
    #config.Site.storageSite = 'T2_CH_CERN'
    print "Storage site = %s" % outDir.split(":")[0]
    config.Site.storageSite = outDir.split(":")[0]
    
    return config

def submitJobs(dataset, iDataset, unitsPerJob, year, outDir, maxEvents, no_submit):
    """Submit crab jobs to produce the Baobab ntuple from a provided dataset."""
    crab_config = getCrabConfig(dataset, iDataset, unitsPerJob, year, outDir, maxEvents)

    dataset_tier = dataset.split("/")[3]
    if dataset_tier.find("SIM") > 0:
        isMC = True
    else:
        isMC = False
    if isMC:
        crab_cfg_file = "crab_" + str(year) + "_" + dataset.split("/")[1] + ".py"
    else:
        crab_cfg_file = "crab_" + str(year) + "_" + dataset.split("/")[1] + "_" + dataset.split("/")[2] + ".py"

    f = open(crab_cfg_file, "w")
    f.write(str(crab_config))
    f.close()

    if no_submit:
        print "Crab configuration file %s created." % crab_cfg_file
        return

    try:
        res = crabCommand('submit', config = crab_config)
        print res
    except HTTPException, e:
        print e.headers

def eos_to_local_path(path):
    myusername = pwd.getpwuid(os.getuid())[0]
    print "eos_to_local_path=%s" % path
        
    if path.find("/store/") == 0: #path: /store/...
        if not eos_mount_point:
            raise RuntimeError("Error: cannot proceed because of the EOS file system mounting failure.")
        return eos_mount_point.rstrip("/") + path
    elif path.find("/eos/") == 0: #path: /eos/...
        #an eos path
        if not eos_mount_point:
            raise RuntimeError("Error: cannot proceed because of the EOS file system mounting failure.")
        return path
    else:
        #not recognised as an eos path
        return path

def eos_to_eos_path(path):
    if path.find("/store/") == 0: #path: /store/...
        return path
    elif path.find("/eos/") == 0: #path: /eos/...
        path = path[path.find("/store"):len(path)]
        return path

def GetNTupleSubDirectories(directory):
    subDirectory=""
    return subDirectory

def catalog_make_ntuple_catalog(dataset, ntuple_dir):

    datatype=0
    ntuple_subdir=0
    
    prim_dataset = dataset.split("/")[1]
    reco_dataset = dataset.split("/")[2]
    dataset_tier = dataset.split("/")[3]
    if dataset_tier.find("SIM") > 0:
        datatype = 'mc'
    else:
        datatype = 'data'

    ntuple_dir_path = eos_to_local_path(ntuple_dir)

    if not ntuple_dir_path:
        sys.stderr("\nNo EOS mount point found. A mount point is needed to access the directory\n")
        return None
    print "ntuple_dir_path=%s" % ntuple_dir_path

    cat_dir_path = os.path.dirname(os.path.dirname(ntuple_dir_path + "/")) + "/Catalogs"
    print "\ncat_dir_path=%s" % cat_dir_path

    # Find out the ntuple subdirectory here
    # can use this to make catalogs for different data-taking eras?
    #ntuple_subdir = GetNTupleSubDirectories(cat_dir_path)
    #ntuple_dir_path += "/" + ntuple_subdir

    #creates directory if it does not exist as it should
    try:
        os.mkdir(cat_dir_path)
    except OSError:
        pass



    def write_file_header(fout, prim_dataset, datatype, processed_events, processed_files, lumi, xsec):
        if xsec:
            xsec_str = "# sample xsec: %s\n" % xsec
        else:
            xsec_str = ""
        #endif
        fout.write('''# primary dataset: %s
# data type: %s
# processed events: %s
# processed files: %s
# lumi: %s
%s
''' % (prim_dataset, datatype, str(processed_events or ''), str(processed_files or ''), str(lumi or ''), xsec_str))
    #enddef
        
    cat_alltasks = cat_dir_path + "/Baobabs-%s-all.txt" % prim_dataset
    print "cat_alltasks=%s" % cat_alltasks

    fout_thistask = open(cat_alltasks, "w")

    nevents=-1
    nfiles=-1
    lumi=-1
    xsec=0
    write_file_header(fout_thistask, prim_dataset, datatype, nevents, nfiles, lumi, xsec)

    task_id=0
    date=""
    nevents=-1
    nfiles=-1
    lumi=None

    fout_thistask.write('''
# task id: %d
# task submission date: %s
# reco dataset: %s
# task nevents: %d
# task nfiles: %d
# task lumi: %s /pb\n
'''     % (task_id, date, reco_dataset, nevents, nfiles, str(lumi)))
 
    # looks for all files within the subfolder of primary dataset (doesn't discriminate among eras currently)
    files = glob.glob(ntuple_dir_path +"/"+ prim_dataset+"/*/*/*/ntupleOut_*.root")
    print "Number of root files = %s" % len(files)
    for ntf in files:
        
        statinfo = os.stat(ntf)
        eos_path = eos_to_eos_path(ntf)
        #print eos_path
        fout_thistask.write("%s\n" % (eos_path))


    try:
        fout_thistask.close()
        sys.stdout.write("\nNtuple catalog written to %s\n" % (cat_alltasks))
    except IOError, e:
        sys.stderr.write("\nError. Failed to write new catalog file for primary dataset %s to include ntuples of task %d.\n" % (prim_dataset, task_id))
        return None

    return
#enddef catalog_make_ntuple_catalog

def main(arguments):

    parser = argparse.ArgumentParser(description='Launches Baobab nutple production on the GRID.')
    parser.add_argument('dataset_catalogs', nargs='+', help='Catalog files containing the list of CMS datasets to process')
    parser.add_argument('--max-events', type=int, action='store', default=-1, help='Specify the maximum number of events to process for each dataset. The default is to process to full dataset.')
    parser.add_argument('--unitsPerJob', type=int, action='store', default = 100000, help='Specify the number of units per job (based on data splitting)')
    parser.add_argument('--year', type=int, action='store', default = 2016, help='Select year of data/MC to process')
    parser.add_argument('--no-submit', action='store_true', help='With this option the crab configuration files are created but the crab tasks are not submitted')
    parser.add_argument('--make-catalogs', action='store_true', help='Make catalog files. Not fully implemented. Mostly for MC.')
    args = parser.parse_args()

    outDirLine = re.compile(r'#\s*output directory[:\s=]+\s*([^\s]+)')
    commentLine = re.compile(r'^\s*^#')
    
    for dataset_catalog in args.dataset_catalogs:
        try:
            f = open(dataset_catalog)
        except IOError, e:
            print e
            continue
        outDir = None

        #Run through the file and look for the output directory
        for dataset in f:
            m = outDirLine.match(dataset)
            if m and len(m.groups()) > 0:
                outDir = m.groups()[0]
            elif not outDir:
                sys.stderr.write('''Fatal error while processing dataset catalog '%s'.
                    Output directory line is missing from the input dataset catalog.
                    The line should be before any dataset name and should have the following format:
                    # output directory: /store/group/phys_smp/AnalysisFramework/Baobab/.../ntuples.
                    ''' % dataset_catalog)
                break

        if(len(outDir.split(":")) < 2):
            print "Please put the storage site in the output directory argument as:\n output directory: site:/store/user/..."
            exit(1)

        f.seek(0)
        if not args.make_catalogs:
            iDataset=0
            for dataset in f:
                dataset=dataset.strip()
                if len(dataset) == 0 or commentLine.match(dataset):
                    continue
                if checkDataSetName(dataset):
                    submitJobs(dataset, iDataset, args.unitsPerJob, args.year, outDir, args.max_events, args.no_submit)
                iDataset+=1

        f.seek(0)
        if args.make_catalogs:
            iDataset=0
            for dataset in f:
                dataset=dataset.strip()
                if len(dataset) == 0 or commentLine.match(dataset):
                    continue
                if checkDataSetName(dataset):
                    print "\ndataset = %s" % dataset
                    print "outDir = %s" % outDir
                    catalog_make_ntuple_catalog(dataset, outDir.split(":")[1])
                iDataset+=1
            
if __name__ == '__main__':
    main(sys.argv[1:])

